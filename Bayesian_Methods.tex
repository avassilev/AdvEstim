% !TeX spellcheck = en_GB
\documentclass[10pt]{beamer}
\usetheme{CambridgeUS}
%\usetheme{Boadilla}
\definecolor{myred}{RGB}{163,0,0}
%\usecolortheme[named=blue]{structure}
\usecolortheme{dove}
\usefonttheme[]{professionalfonts}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{gensymb}
\usepackage{verbatim} 
\usepackage{paratype}
\usepackage{mathpazo}
\usepackage{listings}
\lstset{language=R}

\usepackage{tikz}
\usetikzlibrary{matrix}

\DeclareMathOperator*{\interior}{int}

% Number theorem environments
\setbeamertemplate{theorem}[ams style]
\setbeamertemplate{theorems}[numbered]

% Reset theorem-like environments so that each is numbered separately
\usepackage{etoolbox}
\undef{\definition}
\theoremstyle{definition}
\newtheorem{definition}{\translate{Definition}}
\newtheorem{Fact}{\translate{Fact}}

% Change colours for theorem-like environments
\definecolor{mygreen1}{RGB}{0,96,0}
\definecolor{mygreen2}{RGB}{229,239,229}
\setbeamercolor{block title}{fg=white,bg=mygreen1}
\setbeamercolor{block body}{fg=black,bg=mygreen2}



\alt<presentation>
{\lstset{%
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\slshape\color{green!50!black},
  frame = single,  
  keywordstyle=\bfseries\color{blue!50!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  %escapechar=\#,
  showstringspaces = false,
  showtabs = false,
  tabsize = 2,
  emphstyle=\color{red}}
}
{
  \lstset{%
    basicstyle=\ttfamily,
    keywordstyle=\bfseries,
    commentstyle=\itshape,
    escapechar=\#,
    showtabs = false,
	tabsize = 2,
    emphstyle=\bfseries\color{red}
  }
} 

\title{R404: Advanced Estimation Techniques}
\subtitle{Topic: Bayesian Methods in Econometrics}
\author{Andrey Vassilev}

\date{2016/2017} 
    
\AtBeginSection{\frame{\usebeamerfont{section title}\centering\insertsection}}

\begin{document}
\maketitle



\begin{frame}[fragile]
\frametitle{Lecture Contents}
\tableofcontents
\end{frame}

\begin{section}{Review: the philosophy of the Bayesian approach}\label{sec:BayesPhil}

\begin{frame}
\frametitle{A classical estimation example}
\begin{itemize}\itemsep1em
\item Consider a sample of iid observations $x_1,\ldots,x_n$ coming from a random variable $\xi \sim N(\mu,\sigma^2)$.
\item We are interested in obtaining an estimate of the mean.
\item A standard approach would be to use the method of maximum likelihood (MML).
\item We construct the likelihood function:
\begin{equation}
\label{eq:NormLik}
\begin{split}
L(x_1,\ldots,x_n|\mu,\sigma^2)= & \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\frac{(x_i-\mu)^2}{\sigma^2}}\\
= & \left(
\frac{1}{2\pi\sigma^2}
\right)^{n/2}e^{-\frac{1}{2}\sum_{i=1}^{n}\frac{(x_i-\mu)^2}{\sigma^2}}.
\end{split}
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A classical estimation example}
\begin{itemize}\itemsep1em
\item According to the MML, we maximize $ L $ w.r.t. $ \mu $.
\item It is well-known (or, if your recollections are hazy, you can derive it) that the solution is given by the estimator $\hat{\mu}=\sum_{i=1}^{n}x_i/n$, i.e. the sample mean.
\item By definition, the statistic $\hat{\mu}$ is a random variable.
\item Consequently, if we keep repeating the experiment and regenerating the $ n $ observations, we'll obtain new samples $\tilde{x}_1,\ldots,\tilde{x}_n$,  $\tilde{\tilde{x}}_1,\ldots,\tilde{\tilde{x}}_n$ etc. and therefore new values of $\hat{\mu}$.
\item For each of those samples the corresponding value $\hat{\mu}$ will be our estimate of the unknown parameter $ \mu $.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A classical estimation example}
\begin{itemize}\itemsep1em
\item When we speak of the statistical properties of $\hat{\mu}$ like unbiasedness or consistency, we are implicitly referring to an ability to repeat the experiment many times or to extend the sample size $ n $ within an experiment.
\item In this context, any probabilistic reasoning about $\hat{\mu}$ is based on a \emph{classical} notion of probability as the theoretical limit of the ratio of occurrences of an event to the total number of trials (i.e. the relative frequency).
\item It can be argued that this notion of probability is ``objective'' -- it derives from an experiment and reflects mechanisms external to an observer.
\item At the same time it is operational only when repeatability is ensured.
	\begin{itemize}\itemsep1em
	\item A football player is allowed to shoot one penalty but misses. Does it matter that he is the best scorer in his team?
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Subjective probability}
\begin{itemize}\itemsep1em
\item The idea of probability is often used in contexts where the classical interpretation as the limit of the relative frequency is not applicable.
\item Consider a person making the following statement: \begin{quote}
The probability that there is life on Mars is 1/1000000.
\end{quote}
\item What is this person trying to say?
	\begin{itemize}\itemsep1em
	\item 
	\item 
	\end{itemize}
\end{itemize}
\end{frame}

\end{section}



\begin{frame}[fragile]
\frametitle{Readings}
Additional readings:\bigskip


\end{frame}

\end{document}



\subsection{Субективна вероятност}

Често пъти обаче понятието ,,вероятност'' се употребва в ситуации,
в които интерпретацията му като теоретична граница на емпиричната
честота на сбъдване на дадено събитие е явно некоректна. Например
когато човек изказва твърдение от вида ,,Вероятността на Марс да
има живот е 1/1000000.'' очевидно не се има предвид нито че при
многократни опити за откриване на живот на Марс такъв ще бъде
откриван средно веднъж на милион опита, нито че ако можеше да
възпроизвеждаме възникването на Вселената безброй много пъти,
отношението на броя реализации, когато на Марс е възникнал живот
към общия брой реализации щеше да клони към 1/1000000. В случая
понятието ,,вероятност'' се използва като измерител за
\emph{субективната} степен на сигурност в правилността на дадено
твърдение. За да подчертаем субективността на понятието
,,вероятност'' при тази употреба на думата, ще отбележим, че така
посочената вероятност може да се разглежда като начин да се измери
справедливата цена на облог за сбъдването на дадено събитие. Ако
вземем изказването ,,Вероятността отбор А да спечели предстоящата
среща с отбор Б е 2/5.'', то може да се разглежда и като
заявление, че съответният човек смята, че за никоя страна няма да
има нечестно предимство, ако той плати 2 лева, за да участва в
залагане, което ще му донесе 5 лева при положение, че отбор А
наистина спечели\footnote{В този случай вероятността е свързана с
понятието ,,шанс''. Ако смятаме, че вероятността за сбъдването на
дадено събитие е $\frac{m}{m+n}$, то свързаният с тази вероятност
шанс е $m:n$ ($m$ към $n$), което може да се интерпретира като
склонност да се заложат $m$ парични единици, за да бъде получена
печалба от $n$ парични единици при сбъдване на съответното събитие
(т.е. да бъдат възстановени заложените $m$ единици и да бъдат
изплатени $n$ допълнително), при това без една от страните да е
облагодетелствана от условията на облога.}. При това положение е
ясно, че е напълно възможно двама души да имат различни субективни
вероятности за сбъдването на дадено събитие, съответно да са
готови да заложат различни суми за това.

Интересно е, че последната употреба на понятието ,,вероятност'',
макар и да изглежда нестрога и слабо свързана с теорията на
вероятностите, всъщност не противоречи на построенията, правени
при математическото изграждане на вероятността и следствията от
нея. Може да се покаже, че при зададено измеримо пространство
$(\Omega,\mathcal{F})$ и наредба $\preceq$ върху елементите на
$\sigma$-алгебрата $\mathcal{F}$ с определени свойства, наредбата
$\preceq$ поражда вероятностна мярка $P$ върху пространството
$(\Omega,\mathcal{F})$. Наредбата $\preceq$ се нарича
\emph{наредба на относително правдоподобие} и нарежда събитията от
$\mathcal{F}$ на основата на това дали са с различна или еднаква
достоверност. Вероятностната мярка $P$, породена от такава
наредба, обикновено се нарича \emph{субективна вероятност}.

При това положение едно вероятностно разпределение може да се
разглежда и като средство за измерване на степента на сигурност
или несигурност по отношение на стойностите на дадена числова
величина. Последното важи и за нашата задача за оценяване на
неизвестната средна $\mu$ на нормалното разпределение. Ако
разглежданията се правят в контекста на субективната вероятност,
съвсем приемливо е да считаме $\mu$ за случайна величина и да се
опитаме да намерим вероятностните \'и характеристики. Използването
на подобен подход разбира се няма нищо общо с обективните свойства
на параметъра $\mu$, за който може да е гарантирано, например от
физически съображения, че е (неизвестна) константа, а не случайна
величина.

\subsection{Същност на бейсовия подход}

Щом за наблюдаваните данни се приема, че са генерирани от някакъв
вероятностен механизъм, а неизвестният параметър (параметри) на
модела се третира като случайна величина поради използването на
субективна вероятност, то за наличната извадка
$\mathbf{x}=(x_1,\ldots,x_n)$ и за параметрите може да се
разглежда съвместна функция на разпределение или, при допускането
за абсолютна непрекъснатост, съвместна плътност. В нашия случай
може да се разглежда съвместната плътност\footnote{Тук и по-долу
за краткост изпускаме от записа наличието на известния $\sigma^2$,
макар че строго погледнато съвместната плътност всъщност би
трябвало да се запише като условна при условие $\sigma^2$ и
останалата налична информация $\mathcal{I}$.} на $\mathbf{x}$ и
$\mu$ $f(\mathbf{x},\mu)$. Тази постановка позволява да
формализираме във вероятностни термини идеята за несигурността по
отношение на стойностите на оценявания параметър, но не преодолява
невъзможността да се възпроизвеждат данните от експеримента.
Понеже повечето от практически интересните ситуации се занимават
със случая, в който извадката $\mathbf{x}$ е фиксирана и е трудно
или невъзможно да се получат още данни, релевантният обект за
изследване е всъщност условната плътност на параметъра при условие
наблюдаваните данни $f(\mu|\mathbf{x})$. Последната може да бъде
получена с прилагане на формулата на Бейс във варианта \'и за
плътности на разпределения на случайни величини: \beq
\label{Bayesformula}
f(\mu|\mathbf{x})=\frac{f(\mathbf{x},\mu)}{f(\mathbf{x})}=\frac{f(\mathbf{x}|\mu)f(\mu)}{f(\mathbf{x})}\eeq
При зададена извадка $\mathbf{x}$ стойността на плътността
$f(\mathbf{x})$ се превръща в известно число и съответно не влияе
върху анализа на $\mu$, а само скалира другите членове в
последното равенство на \eqref{Bayesformula}. Тогава формула
\eqref{Bayesformula} може да бъде записана като \beq
\label{BayesProportional}f(\mu|\mathbf{x})\propto
f(\mathbf{x}|\mu)f(\mu), \eeq където $\propto$ е знак за
пропорционалност.

Записът \eqref{BayesProportional} на формулата на Бейс съдържа
ключовите елементи за анализа. Членът $f(\mathbf{x}|\mu)$ е добре
познатата ни функция на правдоподобие, която в разглеждания от нас
пример се задава с \eqref{NormLik}. Членът $f(\mu)$ се нарича
\emph{априорна плътност} на параметъра $\mu$. Априорната плътност
съдържа предварителната (преди провеждане на експеримента)
информация за стойнoстите на параметъра $\mu$. Тази информация
може да отразява чисто субективни убеждения (подобно на дадените
по-горе примери), но може да бъде формирана например и на база
теоретични резултати или информация от подобни експерименти, които
са били провеждани в миналото. Условната плътност
$f(\mu|\mathbf{x})$ се нарича \emph{апостериорно разпределение} на
$\mu$. Апостериорното разпределение посредством формулата на Бейс
обединява експерименталната\footnote{При допускането, че функцията
на правдоподобие адекватно обобщава експерименталната информация.
Това допускане е известно като \emph{принцип на правдоподобието}.}
и неексперименталната информация за стойностите на параметъра
$\mu$ и именно оттам са влезли в употреба понятия като ,,бейсов
анализ'' и ,,бейсова статистика''. Изразът в
\eqref{BayesProportional} отдясно на знака за пропорционалност
понякога се нарича \emph{ядро} на апостериорната плътност. Ясно е,
че горните принципи остават в сила и за многомерния случай.

Формула \eqref{BayesProportional} може да се разглежда като
формален механизъм за ревизиране на някаква извънекспериментална
информация с помощта на емпирични данни. Алтернативно тя може да
бъде интерпретирана и като начин в емпиричния статистически анализ
да бъде вкарана допълнителна информация като теоретични резултати
или експертен опит. И в двата случая основен проблем е как
наличната информация да бъде кодирана с помощта на априорната
плътност. За това съществуват различни варианти, по-сложните от
които например са базирани на итеративни сесии с групи от
експерти, чиито мнения последователно се ревизират и обобщават. В
най-стандартния случай се действа по-просто, като се избира клас
разпределения според типа параметър (например нормално за
параметър без ограничения, гама за положителен параметър, бета за
параметър между 0 и 1 и т.н.) и от този клас се избира
представител с необходимите характеристики (например подходяща
средна, която да отразява стойността на параметъра, считана от нас
за най-достоверна, и подходяща дисперсия, която да отразява
несигурността ни за достоверността на нашата преценка).

Да илюстрираме изчислението на апостериорната плътност за
разглеждания от нас пример за $N(\mu,\sigma^2)$. Функцията на
правдоподобие \eqref{NormLik} съдържа израза \beq \label{NormLik1}
\begin{split} \sum_{i=1}^{n}(x_i-\mu)^2= &
\sum_{i=1}^{n}\left((x_i-\hat{\mu})-(\mu-\hat{\mu})\right)^2=\sum_{i=1}^{n}(x_i-\hat{\mu})^2-\sum_{i=1}^{n}2(x_i-\hat{\mu})(\mu-\hat{\mu})+n(\mu-\hat{\mu})^2\\=&\sum_{i=1}^{n}(x_i-\hat{\mu})^2-2(\mu-\hat{\mu})\underbrace{\sum_{i=1}^{n}(x_i-\hat{\mu})}_{0}+n(\mu-\hat{\mu})^2=\sum_{i=1}^{n}(x_i-\hat{\mu})^2+n(\mu-\hat{\mu})^2\\
=& \nu s^2+n(\mu-\hat{\mu})^2,
\end{split} \eeq където $\nu=n-1$ и
$s^2=\nu^{-1}\sum_{i=1}^{n}(x_i-\hat{\mu})^2$. Следователно
\eqref{NormLik} може да бъде записано като \beq \label{NormLik2}
L(x_1,\ldots,x_n|\mu,\sigma^2)=\left( \frac{1}{2\pi\sigma^2}
\right)^{n/2}\exp\left(-\frac{1}{2\sigma^2}\left(\nu
s^2+n(\mu-\hat{\mu})^2\right)\right). \eeq Нека априорната ни
информация за $\mu$ да се описва с помощта на априорната плътност
\beq \label{NormPrior}
f(\mu)=\frac{1}{\sqrt{2\pi}\sigma^2_a}\exp\left(
-\frac{1}{2\sigma^2_a}(\mu-\mu_a)^2 \right), \eeq където $\mu_a$ е
априорното очакване на изследователя, а $\sigma_a^2$ е априорната
дисперсия. Тогава обединявайки \eqref{NormLik2} и
\eqref{NormPrior} с помощта на \eqref{BayesProportional},
получаваме \beq \label{NormPosterior} \begin{split}
f(\mu|\mathbf{x})\propto & \exp\left( -\frac{1}{2}\left[
\frac{(\mu-\mu_a)^2}{\sigma_a^2}+\frac{n}{\sigma^2}(\mu-\hat{\mu})^2
\right] \right) \\ \propto & \exp \left( -\left(
\frac{\sigma_a^2+\sigma^2/n}{2\sigma^2_a\sigma^2/n} \right)\left(
\mu-\frac{\hat{\mu}\sigma^2_a+\mu_a\frac{\sigma^2}{n}}{\sigma^2_a+\sigma^2/n}
\right)^2 \right). \end{split} \eeq Следователно апостериорното
разпределение на $\mu$ е нормално с очакване
$$\mathbb{E}[\mu]=\frac{\hat{\mu}\sigma^2_a+\mu_a\sigma^2/n}{\sigma^2_a+\sigma^2/n}=\frac{\hat{\mu}(\sigma^2/n)^{-1}+\mu_a(\sigma_a^2)^{-1}}{(\sigma^2/n)^{-1}+(\sigma^2_a)^{-1}}$$
и дисперсия
$$\mathbb{D}[\mu]=\frac{\sigma^2_a\sigma^2/n}{\sigma^2_a+\sigma^2/n}=\frac{1}{(\sigma^2/n)^{-1}+(\sigma^2_a)^{-1}}.$$

Ако въведем означенията $h_0=(\sigma^2/n)^{-1}$ и
$h_a=(\sigma^2_a)^{-1}$, то $\mathbb{E}[\mu]=(\hat{\mu}h_0+\mu_a
h_a)/(h_0+h_a)$ и $\mathbb{D}[\mu]=\frac{1}{h_0+h_a}$. Параметрите
$h_0$ и $h_a$ се наричат \emph{параметри на точност}. Така
очакването на апостериорното разпределение се оказва претеглена
средна от извадъчната средна и априорното очакване.

\subsection{Бейсов анализ при повече от една извадка с данни}

Теоремата на Бейс може да се използва и за последователно
актуализиране на разполагаемата информация при получаване на нови
извадки с данни. Ако от първоначалния експеримент сме имали
извадка $\mathbf{x_1}$ и априорна плътност $f(\mu)$, то
апостериорната плътност е $f(\mu|\mathbf{x_1}) \propto
f(\mu)f(\mathbf{x_1}|\mu)$. Нека сме получили допълнителна
извадъчна информация под формата на нови наблюдения
$\mathbf{x_2}$. Тогава апостериорната плътност
$f(\mu|\mathbf{x_1})$ може да се използва като априорна за новата
извадка и с помощта на теоремата на Бейс получаваме \beq
\label{Bayesupdate1} f(\mu|\mathbf{x_1},\mathbf{x_2}) \propto
f(\mu|\mathbf{x_1})f(\mathbf{x_2}|\mu),\eeq където
$f(\mu|\mathbf{x_1},\mathbf{x_2})$ означава апостериорната
плътност, получена при обединяването на двете извадки. Формула
\eqref{Bayesupdate1} може да бъде записана и като \beq
\label{Bayesupdate2} f(\mu|\mathbf{x_1},\mathbf{x_2}) \propto
f(\mu)f(\mathbf{x_1}|\mu)f(\mathbf{x_2}|\mu). \eeq Тъй като
$f(\mathbf{x_1}|\mu)f(\mathbf{x_2}|\mu)$ е функцията на
правдоподобие за $\mu$, основана на обединената информация от
двете извадки $\mathbf{x_1}$ и $\mathbf{x_2}$, за новата
апостериорна плътност се получава един и същи резултат, независимо
дали сме получавали извадъчната информация последователно или още
в началото сме получили пълната извадка $(\mathbf{x_1},
\mathbf{x_2})$. Ясно е също, че тази процедура се обобщава
директно за случая на повече от две извадки.

\subsection{Маргинални и условни апостериорни плътности}

Както беше отбелязано по-горе, бейсовият подход остава валиден и
ако оценяваме вектор от параметри $\bs{\theta}$, като в този
случай съвместната апостериорна плътност означаваме с
$f(\bs{\theta}|\mathbf{x})$. При това положение могат да възникнат
ситуации, когато ни интересува само някакво подмножество от
параметрите $\bs{\theta}$, т.е. за
$\bs{\theta}=\left(\bs{\theta_1'}:\bs{\theta_2'}\right)$ искаме да
отделим само апостериорната информация за $\bs{\theta_1}$ или с
други думи искаме да получим маргиналната апостериорна плътност на
вектора $\bs{\theta_1}$. Тази маргинална апостериорна плътност
може да бъде получена по следния начин: \beq
\label{MargPost}f(\bs{\theta_1}|\mathbf{x})=\int_{R_{\bs{\theta_2}}}f(\bs{\theta_1},\bs{\theta_2}|\mathbf{x})d\bs{\theta_2}=\int_{R_{\bs{\theta_2}}}f(\bs{\theta_1}|\bs{\theta_2},\mathbf{x})f(\bs{\theta_2|\mathbf{x}})d\bs{\theta_2},
\eeq където $R_{\bs{\theta_2}}$ означава областта на съществуване
на $\bs{\theta_2}$, а $f(\bs{\theta_1}|\bs{\theta_2},\mathbf{x})$
е условната апостериорна плътност на $\bs{\theta_1}$ при зададени
$\bs{\theta_2}$ и $\mathbf{x}$. Изразът след второто равенство в
\eqref{MargPost} показва, че маргиналната апостериорна плътност
$f(\bs{\theta_1}|\mathbf{x})$ може да се разглежда като
усредняване на стойностите на условната апостериорна плътност
$f(\bs{\theta_1}|\bs{\theta_2},\mathbf{x})$ с използване на
маргиналната апостериорна плътност за $\bs{\theta_2}$,
$f(\bs{\theta_2}|\mathbf{x})$, като теглова функция. С помощта на
интегрирането в \eqref{MargPost} елиминираме информацията за
параметрите, които не ни интересуват, за да остане само
апостериорната информация за релевантните параметри.

\subsection{Точкови оценки на параметри при бейсовия подход}

Макар че апостериорната плътност обобщава всичката информация за
интересуващите ни параметри (извадъчна и извънекспериментална), с
която разполагаме, често пъти е необходимо да можем да
характеризираме неизвестните параметри с конкретни стойности, т.е.
да получим точкови оценки за тези параметри. При наличието на
апостериорната плътност като източник на информация за параметъра
$\bs{\theta}$, за точковата оценка
$\bs{\hat{\theta}}=\bs{\hat{\theta}}(\mathbf{x})$ е най-подходящо
да бъде получена в контекста на теорията за вземане на решения.
Съгласно принципите на тази теория изследователят трябва да има
\emph{функция на загубите}, $L(\bs{\theta},\bs{\hat{\theta}})$,
която да отразява колко ,,вредни'' са отклоненията на оценката
$\bs{\hat{\theta}}$ от истинската стойност $\bs{\theta}$. Тъй като
в нашия случай $\bs{\theta}$ се разглежда като случайна величина,
съответно $L(\bs{\theta},\bs{\hat{\theta}})$ е също случайна,
макар че наблюденията $\mathbf{x}$ са фиксирани.

Понеже апостериорната плътност отразява всичката налична
информация за възможните изходи на $\bs{\theta}$, логично е да
претеглим стойностите на функцията на загубите с апостериорната
плътност, за да получим очакваната загуба при използването на
конкретна точкова оценка $\bs{\hat{\theta}}$. При това положение
оптималната точкова оценка е онази, която минимизира
математическото очакване на функцията на загубите: \beq
\label{ExpLoss}
\bs{\hat{\theta^*}}=\min_{\bs{\hat{\theta}}}\mathbb{E}[L(\bs{\theta},\bs{\hat{\theta}})]=\min_{\bs{\hat{\theta}}}\int_{R_{\bs{\theta}}}
L(\bs{\theta},\bs{\hat{\theta}})f(\bs{\theta}|\mathbf{x})
d\bs{\theta}. \eeq Очакването на функцията на загубите се нарича
\emph{риск} или \emph{функция на риска}. Тук имплицитно се
предполага, че очакването е крайно и че минимумът съществува.

Като пример за типа точкова оценка, която се получава при
използване на конкретна функция на загубите, нека разгледаме
квадратична функция на загубите
$L(\bs{\theta},\bs{\hat{\theta}})=(\bs{\theta}-\bs{\hat{\theta}})'C(\bs{\theta}-\bs{\hat{\theta}})$,
където $C$ е зададена положително определена симетрична матрица.
Тогава апостериорното очакване на
$L(\bs{\theta},\bs{\hat{\theta}})$ ще бъде \beq
\label{QuadExpLoss}\begin{split}
\mathbb{E}[L(\bs{\theta},\bs{\hat{\theta}})]=&\mathbb{E}[(\bs{\theta}-\bs{\hat{\theta}})'C(\bs{\theta}-\bs{\hat{\theta}})]\\=&\mathbb{E}[((\bs{\theta}-\mathbb{E}[\bs{\theta}])-(\bs{\hat{\theta}}-\mathbb{E}[\bs{\theta}]))'C((\bs{\theta}-\mathbb{E}[\bs{\theta}])-(\bs{\hat{\theta}}-\mathbb{E}[\bs{\theta}]))]\\=&\mathbb{E}[(\bs{\theta}-\mathbb{E}[\bs{\theta}])'C(\bs{\theta}-\mathbb{E}[\bs{\theta}])]+(\bs{\hat{\theta}}-\mathbb{E}[\bs{\theta}])'C(\bs{\hat{\theta}}-\mathbb{E}[\bs{\theta}]),
\end{split} \eeq където в последното равенство вторият член от
сумата е нестохастичен и излиза извън очакването. Същевременно
именно вторият член зависи от $\bs{\hat{\theta}}$ и съответно
минимумът на \eqref{QuadExpLoss} зависи само него. Ясно е, че за
положително определена $C$ минимум се достига при
$\bs{\hat{\theta^*}}=\mathbb{E}[\bs{\theta}]$. Тоест за
квадратична функция на загубите оптималната точкова оценка се дава
от математическото очакване на апостериорното разпределение.

Аналогично при съответните условия може да се покаже, че за
функция на загубите от вида
$L(\bs{\theta},\bs{\hat{\theta}})=\|\bs{\theta}-\bs{\hat{\theta}}\|$
(абсолютна грешка) оптималната точкова оценка е медианата на
апостериорното разпределение. Ако функцията на загубите е от вида
\begin{equation*} L(\bs{\theta},\bs{\hat{\theta}})=\left\{ \begin{aligned} &0,~\bs{\hat{\theta}}=\bs{\theta}\\ &1,~\bs{\hat{\theta}}\neq\bs{\theta} \end{aligned}\right.
,\end{equation*} то оптималната точкова оценка е модата на
апостериорното разпределение.

\subsection{Бейсови области на достоверност}\label{BayesRegion}

Бейсовият аналог на доверителните интервали в класическия анализ
са т.нар. \emph{области на достоверност}. При положение, че
разполагаме с апостериорната плътност $f(\bs{\theta}|\mathbf{x})$,
можем да изчислим вероятността векторът на параметрите
$\bs{\theta}$ да лежи в дадена подобласт $\bar{R}$ на
параметричното пространство: \beq \label{CredReg} P(\bs{\theta}\in
\bar{R}|\mathbf{x})=\int_{\bar{R}}f(\bs{\theta}|\mathbf{x})d\bs{\theta}.\eeq

Задачата може да бъде разгледана и в обратната формулировка: ако
сме фиксирали вероятността $P(\bs{\theta}\in \bar{R}|\mathbf{x})$,
да се намери област $\bar{R}$, за която \eqref{CredReg} е
изпълнено. При това положение обаче за областта $\bar{R}$ не е
гарантирано, че е единствена. Ако апостериорната плътност е
унимодална, за някои случаи единствена бейсова област $\bar{R}$
може да бъде получена като се наложи допълнителното изискване
стойностите на апостериорната плътност за $\bar{R}$ да не бъдат
по-малки, отколкото стойностите \'и за всяка друга област,
удовлетворяваща \eqref{CredReg} за избраната вероятност
(\emph{критерий за максимална апостериорна плътност}). За
унимодална симетрична плътност в едномерния случай например
областта на достоверност е интервал със среда, центрирана върху
модата на разпределението, за който стойностите на апостериорната
плътност са най-големи.

Обръщаме специално внимание на факта, че интерпретацията на
бейсовите области на достоверност е съвсем различна в сравнение с
класическите доверителни интервали. В контекста на класическата
статистика доверителният интервал е случаен, понеже зависи от
реализациите на данните и съответно вероятностните твърдения се
отнасят до вероятността, с която този случаен интервал ще покрие
(неизвестната) стойност на параметъра. В бейсов контекст областта
на достоверност е напълно детерминистична и вероятностните
твърдения се отнасят до вероятността, с която случайният параметър
попада във фиксираната област на достоверност.

\subsection{Маргинални разпределения на наблюденията}

За случаите, когато ни интересуват маргиналните плътности
$f(\mathbf{x})$ на наблюденията, те могат да бъдат получени по
следния начин: \beq
\label{MargObs}f(\mathbf{x})=\int_{R_{\bs{\theta}}}f(\bs{\theta},\mathbf{x})d\bs{\theta}=\int_{R_{\bs{\theta}}}f(\mathbf{x}|\bs{\theta})f(\bs{\theta})d\bs{\theta}.\eeq
Изразът след второто равенство в \eqref{MargObs} показва, че
маргиналната плътност на наблюденията може да се разглежда като
средна на функцията на правдоподобие при използване на априорната
плътност като теглова функция.

\subsection{Прогнозни плътности на разпределения}

При разглеждането на задачи за прогнозиране в бейсов контекст един
от важните въпроси е какво е разпределението на още ненаблюдавани
събития $\mathbf{\tilde{x}}$. Разглеждаме съвместната плътност на
нерегистрираните данни $\mathbf{\tilde{x}}$ и параметрите
$\bs{\theta}$ при условие вече наблюдаваните данни $\mathbf{x}$:
\beq \label{JntData}
f(\mathbf{\tilde{x}},\bs{\theta}|\mathbf{x})=f(\mathbf{\tilde{x}}|\bs{\theta},\mathbf{x})f(\bs{\theta}|\mathbf{x}).
\eeq Тогава прогнозната плътност на $\mathbf{\tilde{x}}$ при
условие $\mathbf{x}$ се получава след интегриране в
\eqref{JntData} по $\bs{\theta}$: \beq \label{MargForec}
f(\mathbf{\tilde{x}}|\mathbf{x})=\int_{R_{\bs{\theta}}}f(\mathbf{\tilde{x}},\bs{\theta}|\mathbf{x})d\bs{\theta}=\int_{R_{\bs{\theta}}}f(\mathbf{\tilde{x}}|\bs{\theta},\mathbf{x})f(\bs{\theta}|\mathbf{x})d\bs{\theta}.
\eeq Последният израз в \eqref{MargForec} означава, че прогнозната
плътност може да се интерпретира и като математическото очакване
на условната прогнозна плътност по отношение на апостериорната
плътност.

\subsection{Точкови прогнози}

Получената в предишната секция прогнозна плътност
$f(\mathbf{\tilde{x}}|\mathbf{x})$ може да се използва, за да
бъдат направени точкови прогнози за бъдещи данни. Принципът на
получаване е същият, както за точковите оценки. Ако
$\mathbf{\hat{x}}$ е точковата прогноза за ненаблюдаваните данни
$\mathbf{\tilde{x}}$, то е необходима функция на загубите
$L(\mathbf{\tilde{x}},\mathbf{\hat{x}})$, с помощта на която можем
да запишем функция на риска и да минимизираме последната по
$\mathbf{\hat{x}}$ т.е. \beq \label{PointForec}
\min_{\mathbf{\hat{x}}}\int_{R_{\mathbf{\tilde{x}}}}L(\mathbf{\tilde{x}},\mathbf{\hat{x}})f(\mathbf{\tilde{x}}|\mathbf{x})d\mathbf{\tilde{x}}.
\eeq Тук $R_{\mathbf{\tilde{x}}}$ означава областта на изменение
на $\mathbf{\tilde{x}}$. Решението на \eqref{PointForec}, ако
съществува, се явява оптимална точкова прогноза за
$\mathbf{\tilde{x}}$. И в този случай важат изводите за типа
точкови прогнози, получавани при определен вид функция на
загубите: очакването на $f(\mathbf{\tilde{x}}|\mathbf{x})$ за
квадратична функция на загубите, медианата за абсолютна грешка и
т.н.

\subsection{Прогнозни области}

Ако имаме прогнозната плътност $f(\mathbf{\tilde{x}}|\mathbf{x})$,
бихме могли да изчислим вероятността бъдещите наблюдения
$\mathbf{\tilde{x}}$ да попаднат в дадена подобласт $\bar{R}$ на
областта им на изменение: \beq
\label{CredRegForec}P(\mathbf{\tilde{x}}\in
\bar{R}|\mathbf{x})=\int_{\bar{R}}f(\mathbf{\tilde{x}}|\mathbf{x})d\mathbf{\tilde{x}}.
\eeq Съвсем аналогично на областите на достоверност, разгледани в
секция \ref{BayesRegion}, можем да решаваме и обратната задача,
при която за зададена вероятност $P(\mathbf{\tilde{x}}\in
\bar{R}|\mathbf{x})$ се търси област $\bar{R}$ (\emph{прогнозна
област}), която удовлетворява \eqref{CredRegForec}. И в този
случай единствеността на такава област не е гарантирана изобщо, но
ако прогнозната плътност $f(\mathbf{\tilde{x}}|\mathbf{x})$ е
унимодална и допълнително наложим изискването за област с
,,максимална прогнозна плътност'', можем да получим единствена
прогнозна област.

\subsection{Бейсова проверка на хипотези}

В контекста на бейсовата статистика проверката на хипотези
представлява вероятностно сравняване на \emph{равнопоставени}
алтернативи, поради което понятието за нулева хипотеза няма
особеното значение, което му се придава в класическите
изследвания. В общия случай разглеждаме твърдения $H_0$ и $H_1$,
които се отнасят до различни модели $M_0$ и $M_1$ за данните
$\mathbf{x}$. За модел $M_i,~i=1,2,$ съответната функция на
правдоподобие е $f_i(\mathbf{x}|\bs{\theta_i})$, а априорната
плътност е $f_i(\bs{\theta_i})$. Тук долните индекси $i$ в
правдоподобията и априорните плътности са използвани, за да
отчетем възможността да се сравняват хипотези за различни
механизми на генериране на данните, съответно с различни априорни
очаквания. Това не изключва възможността да се разглеждат
по-прости случаи, например когато параметрите $\bs{\theta_0}$ и
$\bs{\theta_1}$, в съответствие с хипотези $H_0$ и $H_1$ се
изменят в различни области на едно и също параметрично
пространство.

При условие, че е верен модел $M_i$, маргиналната плътност на
данните (\emph{маргинална функция на правдоподобие}) е \beq
\label{MargLik}
f(\mathbf{x}|M_i)=\int_{R_{\bs{\theta_i}}}f_i(\mathbf{x}|\bs{\theta_i})f_i(\bs{\theta_i})d\bs{\theta_i}.\eeq

Тогава \emph{множител на Бейс} се дефинира като отношението между
маргиналните функции на правдоподобие за двата модела: \beq
\label{BayesFactor}B_{01}(\mathbf{x})=\frac{f(\mathbf{x}|M_0)}{f(\mathbf{x}|M_1)}.\eeq
Интуитивно множител на Бейс, който е по-голям от 1, може да се
интерпретира като индикация за по-голяма достоверност на модел
$M_0$ (или съответно $H_0$) в сравнение с $M_1$ $(H_1)$.

При анализ в бейсова схема може да има априорна информация и за
достоверността на самите модели, която да се отчете при
сравняването на хипотезите. Ако $P(M_i)$ е априорната вероятност
на модел $M_i$, то можем да изчислим апостериорната вероятност на
на този модел при условие данните $\mathbf{x}$ с помощта на
формулата на Бейс: \beq
\label{ModelPost}P(M_i|\mathbf{x})=\frac{P(M_i)f(\mathbf{x}|M_i)}{\sum_{j=0}^{1}P(M_j)f(\mathbf{x}|M_j)}.
\eeq Тогава \emph{апостериорният шанс} в полза на модел $M_0$
сравнен с $M_1$ се дефинира като отношението на апостериорните
вероятности на двата модела: \beq
\label{PostOdds}K_{01}(\mathbf{x})=\frac{P(M_0|\mathbf{x})}{P(M_1|\mathbf{x})}=\frac{P(M_0)}{P(M_1)}\times
\frac{f(\mathbf{x}|M_0)}{f(\mathbf{x}|M_1)}=\frac{P(M_0)}{P(M_1)}B_{01}(\mathbf{x}).
\eeq Уравнение \eqref{PostOdds} показва защо $B_{01}(\mathbf{x})$
се нарича множител на Бейс: това е величината, с която умножаваме
\emph{априорния шанс} $P(M_0)/P(M_1)$, за да го коригираме с
информацията от данните и да стигнем до апостериорния шанс. С
други думи, множителят на Бейс показва дали наблюдаваните данни
увеличават или намаляват достоверността на единия модел в
сравнение с другия.

Макар че горните разглеждания бяха дадени в термините на
сравняване на модели, те се пренасят без изменения и за случая на
сравняване на хипотези. Това означава, че ако сме изчислили
апостериорния шанс $K_{01}(\mathbf{x})$ при сравняване на хипотеза
$H_0$ с $H_1$, с негова помощ можем да определим коя хипотеза е
по-вероятна. При отсъствието на априорна информация в полза на
едната от двете хипотези, те могат да се разглеждат като
равновероятни и съответно априорният шанс е единица. Тогава
апостериорният шанс става равен на множителя на Бейс и
$B_{01}(\mathbf{x})$ е достатъчен, за да се направи извод за
сравняваните хипотези.

Ако задачата е в явен вид да се приеме или отхвърли хипотезата
$H_0$ в сравнение с $H_1$, използването само на апостериорния
шанс\footnote{Например като се прилагат правила от типа ,,Ако
$K_{01}(\mathbf{x})\geq 1$ приемаме $H_0$, в противен случай
отхвърляме $H_0$.''} е относително непрецизно средство за вземане
на решение. В такива случаи се препоръчва да се работи от гледна
точка на теорията за вземане на решения, като се запише в явен вид
функция на загубите $L(H_i,\hat{H}_j)$, където $H_i$ е истинското
състояние, а $\hat{H}_j$ е приетата от изследователя хипотеза.
След това можем да изчислим очакването на функцията на загубите
при условие, че е взето решение $\hat{H}_j$ и да приемем онова
$\hat{H}_j$, за което съответната очаквана загуба е най-малка.

\subsection{Някои общи бележки върху бейсовите методи}

\subsubsection{Информативни и неинформативни априорни плътности}

Дотук в изложението приемахме, че априорните плътности са избрани
така, че да отразяват наличната неекспериментална информация. В
този смисъл тези плътности могат да бъдат наречени
\emph{информативни}. Можем да си зададем въпроса как да провеждаме
бейсовия анализ в ситуации, за които не разполагаме с
предварителна информация. Интуитивно е да се приеме, че в такива
случаи различните възможни стойности на изследваните параметри са
равновероятни. За съжаление подобна трактовка води до проблеми в
непрекъснатия случай: ако например сме решили да изберем
априорната плътност като плътност на равномерно разпределение в
някакъв интервал, тогава фиксирането на носителя на равномерното
разпределение всъщност представлява вкарване на информация в
анализа, доколкото извън него слагаме вероятност нула на
съответните стойности на параметъра. При това положение понятието
за \emph{неинформативна} плътност става проблематично. Може да се
покаже, че при налагането на определени изисквания за
неинформативност плътностите, които отговарят на тях са
несобствени в смисъл, че те не се интегрират до крайно число.

При положение, че интеграл от неинформативна ,,плътност'' е
разходящ, такъв обект разбира се не се явява плътност на
вероятностно разпределение в истинския смисъл на думата. Въпреки
всичко функции с такива свойства могат да бъдат полезни, ако
апостериорната плътност остава истинска вероятностна плътност или,
с други думи, ако ядрото \'и е с краен интеграл. Тогава бейсовият
анализ може да бъде провеждан и съответно изводите от него ще
бъдат само на база информация, получена от наблюдаваните данни.
Често пъти изводи или оценки, получени при използването на
неинформативни априорни плътности за достатъчно големи извадки
съвпадат с тези, получени от класическия статистически анализ.
Макар че тук няма да разглеждаме бейсов анализ с неинформативни
плътности, заради пълнотата на изложението отбелязваме това важно
направление в бейсовата статистика.

\subsubsection{Монте Карло симулации с марковски вериги}

Ясно е, че при степента на общност на горните резултати изборът на
априорна плътност и на функция на правдоподобие може да води до
най-разнообразни апостериорни плътности. В повечето случаи тези
апостериорни плътности не могат да бъдат изследвани аналитично и
се налага да се прибегне до численото им изследване. За съжаление
в случаите на многомерни параметри обработката на тези плътности
дори с помощта на мощни компютри е затруднена, доколкото
симулирането на случайни извадки от тях или интегрирането им
създават големи изчислителни затруднения при използването на
стандартни числени методи. В миналото този проблем е бил
заобикалян с помощта на т.нар. \emph{спрегнати} априорни
плътности, т.е. априорни плътности, за които получената
апостериорна плътност е от същия клас. Избирани са били спрегнати
плътности, които са достатъчно лесни за аналитично изследване или
числена обработка.

С навлизането на достатъчно мощна изчислителна техника през
последните 20 години бейсовият анализ започва да използва клас от
методи за числено симулиране, известни като \emph{Монте Карло
симулации с марковски вериги (МКСМВ)}\footnote{Английският термин
е \emph{Markov Chain Monte Carlo (MCMC)}.}. Те са силно интензивни
от изчислителна гледна точка, но са особено ефективни за целите на
бейсовия анализ. Основната идея на тези методи е да се симулира
достатъчно голяма извадка от апостериорната плътност като
реализация от подходящо построена марковска верига. След това
анализът на свойствата на апостериорното разпределение се прави на
основата на така генерираната извадка. Обръщаме внимание, че
повечето от бейсовите резултати, които могат да бъдат намерени в
специализираната литература, са получени именно с помощта на
МКСМВ, а не аналитично.

\section{Бейсов иконометричен анализ на линейния регресионен модел}

За целите на анализа разглеждаме познатия ни линеен регресионен
модел \beq \label{LinRegMod}
\mathbf{y}=X\bs{\beta}+\mathbf{e},\eeq където разполагаме с $T$
наблюдения и имаме $K$ параметъра. За матрицата $X$ предполагаме,
че е нестохастична\footnote{В случая разсъжденията са аналогични и
за стохастична $X$ при положение, че тя е независима от грешката
$\mathbf{e}$.} и е изпълнено $rank(X)=K$, а за грешката
$\mathbf{e}\in N(\mathbf{0},\sigma^2 I_T)$.

Функцията на правдоподобие за \eqref{LinRegMod} при допускането за
нормалност на грешката е \beq \label{LRMLik} \ell
(\mathbf{y}|\bs{\beta}, \sigma^2)=(2\pi \sigma^2)^{-T/2}\exp
\left[
-\frac{(\mathbf{y}-X\bs{\beta})'(\mathbf{y}-X\bs{\beta})}{2\sigma^2}
\right]. \eeq Съгласно принципите на бейсовия анализ параметрите
на модела $\bs{\beta}$ и $\sigma^2$ се разглеждат като случайни и
съответно за тях трябва да бъде зададена априорна плътност. За
случая избираме следната априорна плътност (представена само с
ядрото си): \beq
\label{LRMPrior}f(\bs{\beta},\sigma)\propto\sigma^{-m}\exp\left[
-\frac{1}{2\sigma^2}[\eta+(\bs{\beta}-\bs{\mu})'\bs{\Psi}^{-1}(\bs{\beta}-\bs{\mu})]
\right], \eeq където $m>K+1$, $\eta>0$ и $\bs{\Psi}$ е симетрична
и положително определена. Тази априорна плътност е пример за
спрегната плътност за съответната функция на правдоподобие.
Изборът на \eqref{LRMPrior} за априорна плътност може да бъде
изяснен по-добре, ако разложим $f(\bs{\beta},\sigma)$ като \beq
\label{DecompLMRPrior}
f(\bs{\beta},\sigma)=f(\bs{\beta}|\sigma)f(\sigma), \eeq където
\beq
\label{CondBeta}f(\bs{\beta}|\sigma)=\frac{1}{(2\pi)^{K/2}\sigma^K
[\det (\bs{\Psi})]^{1/2}}\exp \left[
-\frac{1}{2\sigma^2}(\bs{\beta}-\bs{\mu})'\bs{\Psi}^{-1}(\bs{\beta}-\bs{\mu})\right]\eeq
и \beq \label{SigmaPrior}f(\sigma)\propto \sigma^{-(m-K)}\exp
\left[ -\frac{\eta}{2\sigma^2} \right],\quad \sigma>0. \eeq Тоест
условно по $\sigma$ векторът $\bs{\beta}$ е многомерно нормално
разпределен със средна $\bs{\mu}$ и ковариационна матрица
$\sigma^2\bs{\Psi}$. Разпределението на $\sigma$, съответстващо на
ядрото \eqref{SigmaPrior}, може да се получи като нелинейна
трансформация на гама-разпределена случайна величина.

При използването на априорната плътност \eqref{LRMPrior} и
функцията на правдоподобие \eqref{LRMLik} за апостериорното
разпределение на разглеждания модел се получава следното ядро:
\beq
\label{LRMPost}\begin{split}f(\bs{\beta},\sigma|\mathbf{y})\propto
& \frac{1}{\sigma^{T+m}}\exp\left[
-\frac{1}{2\sigma^2}\left[(T-K)\hat{\sigma}^2+(\bs{\beta}-\mathbf{b})'X'X(\bs{\beta}-\mathbf{b})\right]
\right]\\& \times \exp \left[
-\frac{1}{2\sigma^2}\left[\eta+(\bs{\beta}-\bs{\mu})'\bs{\Psi}^{-1}(\bs{\beta}-\bs{\mu})\right]
\right], \end{split}\eeq където $\hat{\sigma}^2=RSS/(T-K)$ и
$\mathbf{b}$ са оценките по МНМК за дисперсията и вектора на
коефициентите. Алгебрично еквивалентен запис за ядрото в
\eqref{LRMPost} е \beq
\label{LRMPost1}f(\bs{\beta},\sigma|\mathbf{y})\propto\frac{1}{\sigma^{T+m}}\exp
\left[
-\frac{1}{2\sigma^2}[(\bs{\beta}-\bs{\beta_*})'(\bs{\Psi}^{-1}+X'X)(\bs{\beta}-\bs{\beta_*})+\xi]
\right], \eeq където
$$\bs{\beta_*}=(\bs{\Psi}^{-1}+X'X)^{-1}(\bs{\Psi}^{-1}\bs{\mu}+X'X\mathbf{b})$$
и
$$\xi=\eta+(T-K)\hat{\sigma}^2+\bs{\mu}'\bs{\Psi}^{-1}\bs{\mu}+\mathbf{b}'X'X\mathbf{b}-\bs{\beta_*}'(\bs{\Psi}^{-1}+X'X)\bs{\beta_*}.$$

Маргиналното апостериорно разпределение за $\bs{\beta}$ може да
бъде получено стандартно, като \eqref{LRMPost1} се интегрира по
$\sigma$. Неговото ядро има вида \beq
\label{MargPostBeta}\begin{split} f(\bs{\beta}|\mathbf{y})\propto
& \left[ \xi +
(\bs{\beta}-\bs{\beta_*})'(\bs{\Psi}^{-1}+X'X)(\bs{\beta}-\bs{\beta_*})
\right]^{-(T+m-1)/2}\\ \propto & \left[
v+(\bs{\beta}-\bs{\beta_*})'\frac{v}{\xi}(\bs{\Psi}^{-1}+X'X)(\bs{\beta}-\bs{\beta_*})\right]^{-(v+K)/2},
\end{split}\eeq където $v=T+m-K-1$. Полученото ядро е от плътност
на многомерно $t$-разпределение. Случаен вектор с такова
разпределение ще има очакване $\bs{\beta_*}$ за $T+m-K>2$ и
ковариационна матрица $$\left( \frac{\xi}{T+m-K-3}
\right)(\bs{\Psi}^{-1}+X'X)^{-1} \textrm{ за } T+m-K>3.$$ Ако сме
означили $\mathbf{h}=\left[ \frac{v}{\xi}(\bs{\Psi}^{-1}+X'X)
\right]$, то разпределението на $j$-тия компонент на $\bs{\beta}$,
$\beta_j$, може да се получи като се отчете, че
$(\beta_j-\beta_{*j})/[h^{-1}(j,j)]^{1/2}$ има едномерно
$t$-разпределение с $v$ степени на свобода. Тук означението в
знаменателя е използвано за корен квадратен от $j$-тия диагонален
елемент на матрицата $\mathbf{h}^{-1}$.

Може да се покаже, че многомерното $t$-разпределение е затворен
клас по отношение на линейни трансформации (с точност до смяна на
параметрите на разпределението). Също така анализът на
разглеждания модел може да стъпи на това, че \beq
\label{QuadBeta}\frac{T+m-K-1}{K\xi}(\mathbf{B}-\bs{\beta_*})'(\bs{\Psi}^{-1}+X'X)(\mathbf{B}-\bs{\beta_*})\eeq
е разпределена като $F(K,v)$ случайна величина, където с
$\mathbf{B}$ сме означили случаен вектор с ядро на плътността
\eqref{MargPostBeta}. Аналогично, ако $C$ е матрица с размерност
$J\times K$, то \beq \label{TransfQuadBeta}
\frac{T+m-K-1}{J\xi}[C(\mathbf{B}-\bs{\beta_*})]'[C(\bs{\Psi}^{-1}+X'X)^{-1}C']^{-1}[C(\mathbf{B}-\bs{\beta_*})]
\eeq има $F(J,v)$ разпределение. С помощта на тези резултати могат
да се правят изследвания на свойствата на $\bs{\beta}$,
включително да се строят бейсови области и да се проверяват
хипотези.